{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "x9PBfR_MjAAj"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "DJ7Em0ojjAfa"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPaj8dywjAh4",
    "outputId": "f431ba13-360b-41de-8c21-3dabd2b792dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_16 (Conv2D)          (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 30, 30, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 28, 28, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 12, 12, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 12, 12, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 10, 10, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 10, 10, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 5, 5, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 5, 5, 64)          0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               204928    \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 273,066\n",
      "Trainable params: 272,426\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "YecsOUgZjAkK"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FdE9af2kjAml",
    "outputId": "6ab568d6-defb-4b43-e782-72bf6bed07b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 266s 168ms/step - loss: 1.6330 - accuracy: 0.4377 - val_loss: 1.2679 - val_accuracy: 0.5438\n",
      "Epoch 2/50\n",
      "1563/1563 [==============================] - 251s 160ms/step - loss: 1.1561 - accuracy: 0.5910 - val_loss: 0.9170 - val_accuracy: 0.6806\n",
      "Epoch 3/50\n",
      "1563/1563 [==============================] - 254s 162ms/step - loss: 1.0060 - accuracy: 0.6457 - val_loss: 0.8784 - val_accuracy: 0.6886\n",
      "Epoch 4/50\n",
      "1563/1563 [==============================] - 252s 161ms/step - loss: 0.9283 - accuracy: 0.6771 - val_loss: 1.0088 - val_accuracy: 0.6444\n",
      "Epoch 5/50\n",
      "1563/1563 [==============================] - 250s 160ms/step - loss: 0.8742 - accuracy: 0.6948 - val_loss: 0.7596 - val_accuracy: 0.7415\n",
      "Epoch 6/50\n",
      "1563/1563 [==============================] - 250s 160ms/step - loss: 0.8320 - accuracy: 0.7106 - val_loss: 0.7781 - val_accuracy: 0.7256\n",
      "Epoch 7/50\n",
      "1563/1563 [==============================] - 256s 164ms/step - loss: 0.7924 - accuracy: 0.7262 - val_loss: 0.7319 - val_accuracy: 0.7447\n",
      "Epoch 8/50\n",
      "1563/1563 [==============================] - 257s 165ms/step - loss: 0.7670 - accuracy: 0.7339 - val_loss: 0.7877 - val_accuracy: 0.7260\n",
      "Epoch 9/50\n",
      "1563/1563 [==============================] - 248s 159ms/step - loss: 0.7435 - accuracy: 0.7418 - val_loss: 0.9054 - val_accuracy: 0.6797\n",
      "Epoch 10/50\n",
      "1563/1563 [==============================] - 258s 165ms/step - loss: 0.7157 - accuracy: 0.7542 - val_loss: 0.6341 - val_accuracy: 0.7782\n",
      "Epoch 11/50\n",
      "1563/1563 [==============================] - 247s 158ms/step - loss: 0.7001 - accuracy: 0.7580 - val_loss: 0.6368 - val_accuracy: 0.7733\n",
      "Epoch 12/50\n",
      "1563/1563 [==============================] - 247s 158ms/step - loss: 0.6817 - accuracy: 0.7633 - val_loss: 0.6284 - val_accuracy: 0.7786\n",
      "Epoch 13/50\n",
      "1563/1563 [==============================] - 257s 165ms/step - loss: 0.6720 - accuracy: 0.7693 - val_loss: 0.6151 - val_accuracy: 0.7889\n",
      "Epoch 14/50\n",
      "1563/1563 [==============================] - 248s 159ms/step - loss: 0.6509 - accuracy: 0.7760 - val_loss: 0.6106 - val_accuracy: 0.7928\n",
      "Epoch 15/50\n",
      "1563/1563 [==============================] - 259s 166ms/step - loss: 0.6457 - accuracy: 0.7771 - val_loss: 0.7277 - val_accuracy: 0.7520\n",
      "Epoch 16/50\n",
      "1563/1563 [==============================] - 260s 166ms/step - loss: 0.6340 - accuracy: 0.7810 - val_loss: 0.5860 - val_accuracy: 0.7994\n",
      "Epoch 17/50\n",
      "1563/1563 [==============================] - 249s 159ms/step - loss: 0.6232 - accuracy: 0.7852 - val_loss: 0.6079 - val_accuracy: 0.7923\n",
      "Epoch 18/50\n",
      "1563/1563 [==============================] - 249s 159ms/step - loss: 0.6128 - accuracy: 0.7879 - val_loss: 0.6005 - val_accuracy: 0.7887\n",
      "Epoch 19/50\n",
      "1563/1563 [==============================] - 257s 164ms/step - loss: 0.6052 - accuracy: 0.7922 - val_loss: 0.5858 - val_accuracy: 0.7966\n",
      "Epoch 20/50\n",
      "1563/1563 [==============================] - 256s 164ms/step - loss: 0.6017 - accuracy: 0.7921 - val_loss: 0.6217 - val_accuracy: 0.7887\n",
      "Epoch 21/50\n",
      "1563/1563 [==============================] - 246s 158ms/step - loss: 0.5873 - accuracy: 0.7958 - val_loss: 0.6352 - val_accuracy: 0.7810\n",
      "Epoch 22/50\n",
      "1563/1563 [==============================] - 247s 158ms/step - loss: 0.5793 - accuracy: 0.8004 - val_loss: 0.5551 - val_accuracy: 0.8062\n",
      "Epoch 23/50\n",
      "1563/1563 [==============================] - 250s 160ms/step - loss: 0.5730 - accuracy: 0.8028 - val_loss: 0.5480 - val_accuracy: 0.8115\n",
      "Epoch 24/50\n",
      "1563/1563 [==============================] - 249s 159ms/step - loss: 0.5672 - accuracy: 0.8050 - val_loss: 0.5591 - val_accuracy: 0.8063\n",
      "Epoch 25/50\n",
      "1563/1563 [==============================] - 265s 169ms/step - loss: 0.5660 - accuracy: 0.8042 - val_loss: 0.5541 - val_accuracy: 0.8108\n",
      "Epoch 26/50\n",
      "1563/1563 [==============================] - 261s 167ms/step - loss: 0.5583 - accuracy: 0.8066 - val_loss: 0.5393 - val_accuracy: 0.8166\n",
      "Epoch 27/50\n",
      "1563/1563 [==============================] - 261s 167ms/step - loss: 0.5511 - accuracy: 0.8089 - val_loss: 0.5534 - val_accuracy: 0.8071\n",
      "Epoch 28/50\n",
      "1563/1563 [==============================] - 248s 159ms/step - loss: 0.5445 - accuracy: 0.8124 - val_loss: 0.5635 - val_accuracy: 0.8057\n",
      "Epoch 29/50\n",
      "1563/1563 [==============================] - 258s 165ms/step - loss: 0.5386 - accuracy: 0.8134 - val_loss: 0.6048 - val_accuracy: 0.7932\n",
      "Epoch 30/50\n",
      "1563/1563 [==============================] - 247s 158ms/step - loss: 0.5396 - accuracy: 0.8135 - val_loss: 0.5316 - val_accuracy: 0.8171\n",
      "Epoch 31/50\n",
      "1563/1563 [==============================] - 249s 160ms/step - loss: 0.5338 - accuracy: 0.8150 - val_loss: 0.5539 - val_accuracy: 0.8117\n",
      "Epoch 32/50\n",
      "1563/1563 [==============================] - 251s 161ms/step - loss: 0.5289 - accuracy: 0.8175 - val_loss: 0.5341 - val_accuracy: 0.8145\n",
      "Epoch 33/50\n",
      "1563/1563 [==============================] - 250s 160ms/step - loss: 0.5281 - accuracy: 0.8153 - val_loss: 0.6031 - val_accuracy: 0.7988\n",
      "Epoch 34/50\n",
      "1563/1563 [==============================] - 249s 160ms/step - loss: 0.5228 - accuracy: 0.8207 - val_loss: 0.5507 - val_accuracy: 0.8126\n",
      "Epoch 35/50\n",
      "1563/1563 [==============================] - 249s 159ms/step - loss: 0.5145 - accuracy: 0.8223 - val_loss: 0.5233 - val_accuracy: 0.8186\n",
      "Epoch 36/50\n",
      "1563/1563 [==============================] - 249s 159ms/step - loss: 0.5169 - accuracy: 0.8205 - val_loss: 0.5482 - val_accuracy: 0.8098\n",
      "Epoch 37/50\n",
      "1563/1563 [==============================] - 248s 159ms/step - loss: 0.5061 - accuracy: 0.8228 - val_loss: 0.5686 - val_accuracy: 0.8074\n",
      "Epoch 38/50\n",
      "1563/1563 [==============================] - 249s 159ms/step - loss: 0.5070 - accuracy: 0.8262 - val_loss: 0.5234 - val_accuracy: 0.8212\n",
      "Epoch 39/50\n",
      "1563/1563 [==============================] - 259s 165ms/step - loss: 0.5069 - accuracy: 0.8240 - val_loss: 0.5149 - val_accuracy: 0.8232\n",
      "Epoch 40/50\n",
      "1563/1563 [==============================] - 250s 160ms/step - loss: 0.5059 - accuracy: 0.8237 - val_loss: 0.5372 - val_accuracy: 0.8143\n",
      "Epoch 41/50\n",
      "1563/1563 [==============================] - 257s 165ms/step - loss: 0.4944 - accuracy: 0.8293 - val_loss: 0.5579 - val_accuracy: 0.8085\n",
      "Epoch 42/50\n",
      "1563/1563 [==============================] - 259s 166ms/step - loss: 0.4939 - accuracy: 0.8293 - val_loss: 0.5431 - val_accuracy: 0.8183\n",
      "Epoch 43/50\n",
      "1563/1563 [==============================] - 249s 159ms/step - loss: 0.4926 - accuracy: 0.8292 - val_loss: 0.5480 - val_accuracy: 0.8176\n",
      "Epoch 44/50\n",
      "1563/1563 [==============================] - 246s 157ms/step - loss: 0.4889 - accuracy: 0.8314 - val_loss: 0.5430 - val_accuracy: 0.8144\n",
      "Epoch 45/50\n",
      "1563/1563 [==============================] - 247s 158ms/step - loss: 0.4884 - accuracy: 0.8310 - val_loss: 0.5885 - val_accuracy: 0.7971\n",
      "Epoch 46/50\n",
      "1563/1563 [==============================] - 258s 165ms/step - loss: 0.4888 - accuracy: 0.8308 - val_loss: 0.5211 - val_accuracy: 0.8197\n",
      "Epoch 47/50\n",
      "1563/1563 [==============================] - 246s 158ms/step - loss: 0.4847 - accuracy: 0.8313 - val_loss: 0.5255 - val_accuracy: 0.8200\n",
      "Epoch 48/50\n",
      "1563/1563 [==============================] - 246s 157ms/step - loss: 0.4768 - accuracy: 0.8347 - val_loss: 0.5449 - val_accuracy: 0.8132\n",
      "Epoch 49/50\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.4782 - accuracy: 0.8349 - val_loss: 0.5234 - val_accuracy: 0.8229\n",
      "Epoch 50/50\n",
      "1563/1563 [==============================] - 257s 164ms/step - loss: 0.4773 - accuracy: 0.8348 - val_loss: 0.5213 - val_accuracy: 0.8241\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=50, validation_data=(test_images, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9_eNAc0WjApM",
    "outputId": "bc7b0833-aced-4da1-f9ae-5f9b69293de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 10s - loss: 0.5213 - accuracy: 0.8241 - 10s/epoch - 33ms/step\n",
      "Test accuracy: 0.8241000175476074\n",
      "Test loss: 0.5212544798851013\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"Test loss:\", test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tQAbld4pjAto",
    "outputId": "fcc16bea-8992-4438-99a3-e0b8d63d57ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 13s 40ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84      1000\n",
      "           1       0.93      0.90      0.92      1000\n",
      "           2       0.77      0.71      0.74      1000\n",
      "           3       0.75      0.58      0.66      1000\n",
      "           4       0.81      0.85      0.83      1000\n",
      "           5       0.76      0.73      0.74      1000\n",
      "           6       0.76      0.94      0.84      1000\n",
      "           7       0.87      0.87      0.87      1000\n",
      "           8       0.90      0.88      0.89      1000\n",
      "           9       0.86      0.93      0.90      1000\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.82      0.82      0.82     10000\n",
      "weighted avg       0.82      0.82      0.82     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "y_pred = model.predict(test_images)\n",
    "\n",
    "y_pred = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "print(classification_report(test_labels, y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
